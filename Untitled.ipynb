{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ffbcbd1-81b7-4222-ba7c-763713ca49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    \"us-east-1\",\n",
    "    aws_access_key_id = \"ZZ0JLR12PPW4410IW3G9\",\n",
    "    aws_secret_access_key = \"yBUSPjz6OxKcIChDGQ0Cd1I7o9Av4bZYZJYT3CJJ\",\n",
    "    endpoint_url= \"http://rook-ceph-rgw-my-store-rook-ceph.apps.k8spro.nextret.net:8080\",\n",
    "    use_ssl= False,\n",
    "    verify= False\n",
    ")\n",
    "\n",
    "#my_bucket = s3_client.Bucket('ccma-pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d961234-a1db-465e-99ce-44b079f8b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    \"us-east-1\",\n",
    "    aws_access_key_id = \"OHM625HESZAWYZY01OI7\",\n",
    "    aws_secret_access_key = \"Qny9KjC27SfNJ0NPp2nFVXFEEnBKL2LifgrlL9en\",\n",
    "    endpoint_url= \"http://10.254.188.84:80\",\n",
    "    use_ssl= False,\n",
    "    verify= False\n",
    ")\n",
    "\n",
    "#my_bucket = s3_client.Bucket('ccma-pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba131dc-3c4c-4b49-b3a1-edcfdc940229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'ingestion/dades_svc_cloudera/formacio/2020/08/13/T_UE.csv',\n",
       "  'LastModified': datetime.datetime(2023, 4, 5, 12, 27, 17, 751000, tzinfo=tzutc()),\n",
       "  'ETag': '\"c8bab6ec63ac049964be58b65d60fc66\"',\n",
       "  'Size': 81,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = \"cityos\"\n",
    "prefix=\"ingestion/dades_svc_cloudera/formacio/2020/08/13/T_UE.csv\"\n",
    "s3_result =  s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
    "s3_result['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34471643-5a79-469d-9287-d7dad0d3e7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nquery = \"SHOW SESSION\"    \\ncur = conn.cursor()\\ncur.execute(query)\\nrows = cur.fetchall()\\nprint(rows)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''query = \"SET SESSION task_concurrency = 4\"    \n",
    "cur = conn.cursor()\n",
    "cur.execute(query)\n",
    "rows = cur.fetchall()\n",
    "print(rows)\n",
    "'''\n",
    "'''\n",
    "query = \"SHOW SESSION\"    \n",
    "cur = conn.cursor()\n",
    "cur.execute(query)\n",
    "rows = cur.fetchall()\n",
    "print(rows)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cffe308-a09e-4b5b-8858-fc6d1d5ef601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hive'], ['system'], ['tpcds'], ['tpch']]\n"
     ]
    }
   ],
   "source": [
    "from trino.dbapi import trino\n",
    "\n",
    "conn = trino.dbapi.connect(\n",
    "        host='trino.apps.k8spro.nextret.net',\n",
    "        port=32056,\n",
    "        catalog=\"hive\",\n",
    "        user='admin')\n",
    "query = \"SHOW catalogs\"    \n",
    "cur = conn.cursor()\n",
    "cur.execute(query)\n",
    "rows = cur.fetchall()\n",
    "print(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ab640b92-0f2d-4162-8d2f-dd4d4fdbc9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Key': 'enterprise/zapping/queries/insert_incremental_kantar_graella_visualitzacions_new.hql',\n",
       "  'LastModified': datetime.datetime(2023, 5, 9, 8, 46, 31, 597000, tzinfo=tzutc()),\n",
       "  'ETag': '\"f5ee875b4ad1b3e49e374733e628fd9e\"',\n",
       "  'Size': 14609,\n",
       "  'StorageClass': 'STANDARD'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket_name = \"ccma-pre\"\n",
    "prefix=\"enterprise/zapping/queries/insert_incremental_kantar_graella_visualitzacions_new.hql\"\n",
    "s3_result =  s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix, Delimiter = \"/\")\n",
    "s3_result['Contents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "631c8de7-f8db-44e4-ad04-85520062e7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enterprise/zapping/queries/insert_incremental_kantar_graella_visualitzacions_new.hql\n",
      "[]\n",
      "[['adaptive_partial_aggregation_enabled', 'true', 'true', 'boolean', 'When enabled, partial aggregation might be adaptively turned off when it does not provide any performance gain'], ['adaptive_partial_aggregation_unique_rows_ratio_threshold', '0.8', '0.8', 'double', 'Ratio between aggregation output and input rows above which partial aggregation might be adaptively turned off'], ['aggregation_operator_unspill_memory_limit', '4MB', '4MB', 'varchar', 'How much memory should be allocated per aggregation operator in unspilling process'], ['collect_plan_statistics_for_all_queries', 'false', 'false', 'boolean', 'Collect plan statistics for non-EXPLAIN queries'], ['colocated_join', 'true', 'true', 'boolean', 'Use a colocated join when possible'], ['default_filter_factor_enabled', 'false', 'false', 'boolean', 'use a default filter factor for unknown filters in a filter node'], ['determine_partition_count_for_write_enabled', 'false', 'false', 'boolean', 'Determine the number of partitions based on amount of data read and processed by the query for write queries'], ['dictionary_aggregation', 'false', 'false', 'boolean', 'Enable optimization for aggregations on dictionaries'], ['distributed_sort', 'true', 'true', 'boolean', 'Parallelize sort across multiple nodes'], ['enable_coordinator_dynamic_filters_distribution', 'true', 'true', 'boolean', 'Enable distribution of dynamic filters from coordinator to all workers'], ['enable_dynamic_filtering', 'true', 'true', 'boolean', 'Enable dynamic filtering'], ['enable_intermediate_aggregations', 'false', 'false', 'boolean', 'Enable the use of intermediate aggregations'], ['enable_large_dynamic_filters', 'false', 'false', 'boolean', 'Enable collection of large dynamic filters'], ['enable_stats_calculator', 'true', 'true', 'boolean', 'Enable statistics calculator'], ['exchange_compression', 'false', 'false', 'boolean', 'Enable compression in exchanges'], ['execution_policy', 'phased', 'phased', 'varchar', 'Policy used for scheduling query tasks'], ['fault_tolerant_execution_coordinator_task_memory', '2GB', '2GB', 'varchar', 'Estimated amount of memory a single coordinator task will use when task level retries are used; value is used when allocating nodes for tasks execution'], ['fault_tolerant_execution_max_partition_count', '50', '50', 'integer', 'Maximum number of partitions for distributed joins and aggregations executed with fault tolerant execution enabled'], ['fault_tolerant_execution_max_task_split_count', '256', '256', 'integer', 'Maximal number of splits for a single fault tolerant task (count based)'], ['fault_tolerant_execution_min_partition_count', '4', '4', 'integer', 'Minimum number of partitions for distributed joins and aggregations executed with fault tolerant execution enabled'], ['fault_tolerant_execution_min_partition_count_for_write', '50', '50', 'integer', 'Minimum number of partitions for distributed joins and aggregations in write queries executed with fault tolerant execution enabled'], ['fault_tolerant_execution_standard_split_size', '64MB', '64MB', 'varchar', 'Standard split size for a single fault tolerant task (split weight aware)'], ['fault_tolerant_execution_task_memory', '5GB', '5GB', 'varchar', 'Estimated amount of memory a single task will use when task level retries are used; value is used when allocating nodes for tasks execution'], ['fault_tolerant_execution_task_memory_estimation_quantile', '0.9', '0.9', 'double', 'What quantile of memory usage of completed tasks to look at when estimating memory usage for upcoming tasks'], ['fault_tolerant_execution_task_memory_growth_factor', '3.0', '3.0', 'double', 'Factor by which estimated task memory is increased if task execution runs out of memory; value is used allocating nodes for tasks execution'], ['filter_and_project_min_output_page_row_count', '256', '256', 'integer', 'Experimental: Minimum output page row count for filter and project operators'], ['filter_and_project_min_output_page_size', '500kB', '500kB', 'varchar', 'Experimental: Minimum output page size for filter and project operators'], ['filter_conjunction_independence_factor', '0.75', '0.75', 'double', 'Scales the strength of independence assumption for selectivity estimates of the conjunction of multiple filters'], ['force_spilling_join', 'false', 'false', 'boolean', 'Force the usage of spliing join operator in favor of the non-spilling one, even if spill is not enabled'], ['hide_inaccessible_columns', 'false', 'false', 'boolean', 'When enabled non-accessible columns are silently filtered from results from SELECT * statements'], ['ignore_downstream_preferences', 'false', 'false', 'boolean', \"Ignore Parent's PreferredProperties in AddExchange optimizer\"], ['ignore_stats_calculator_failures', 'true', 'true', 'boolean', 'Ignore statistics calculator failures'], ['incremental_hash_array_load_factor_enabled', 'true', 'true', 'boolean', 'Use smaller load factor for small hash arrays in order to improve performance'], ['initial_splits_per_node', '112', '112', 'integer', 'The number of splits each node will run per task, initially'], ['iterative_optimizer_timeout', '3.00m', '3.00m', 'varchar', 'Timeout for plan optimization in iterative optimizer'], ['join_distribution_type', 'AUTOMATIC', 'AUTOMATIC', 'varchar', 'Join distribution type. Possible values: [BROADCAST, PARTITIONED, AUTOMATIC]'], ['join_max_broadcast_table_size', '100MB', '100MB', 'varchar', 'Maximum estimated size of a table that can be broadcast when using automatic join type selection'], ['join_multi_clause_independence_factor', '0.25', '0.25', 'double', 'Scales the strength of independence assumption for selectivity estimates of multi-clause joins'], ['join_partitioned_build_min_row_count', '1000000', '1000000', 'bigint', 'Minimum number of join build side rows required to use partitioned join lookup'], ['join_reordering_strategy', 'AUTOMATIC', 'AUTOMATIC', 'varchar', 'Join reordering strategy. Possible values: [NONE, ELIMINATE_CROSS_JOINS, AUTOMATIC]'], ['late_materialization', 'false', 'false', 'boolean', 'Experimental: Use late materialization (including WorkProcessor pipelines)'], ['legacy_materialized_view_grace_period', 'false', 'false', 'boolean', 'Enable legacy handling of stale materialized views'], ['mark_distinct_strategy', '', '', 'varchar', '. Possible values: [NONE, ALWAYS, AUTOMATIC]'], ['max_drivers_per_task', '', '', 'integer', 'Maximum number of drivers per task'], ['max_hash_partition_count', '100', '100', 'integer', 'Maximum number of partitions for distributed joins and aggregations'], ['max_partial_top_n_memory', '16MB', '16MB', 'varchar', \"Max memory size for partial Top N aggregations. This can be turned off by setting it with '0'.\"], ['max_recursion_depth', '10', '10', 'integer', 'Maximum recursion depth for recursive common table expression'], ['max_reordered_joins', '9', '9', 'integer', 'The maximum number of joins to reorder as one group in cost-based join reordering'], ['max_tasks_waiting_for_execution_per_query', '10', '10', 'integer', 'Maximum number of tasks waiting to be scheduled per query. Split enumeration is paused by the scheduler when this threshold is crossed'], ['max_tasks_waiting_for_node_per_stage', '5', '5', 'integer', 'Maximum possible number of tasks waiting for node allocation per stage before scheduling of new tasks for stage is paused'], ['max_unacknowledged_splits_per_task', '2000', '2000', 'integer', 'Maximum number of leaf splits awaiting delivery to a given task'], ['max_writer_tasks_count', '100', '100', 'integer', 'Maximum number of tasks that will participate in writing data'], ['merge_project_with_values', 'true', 'true', 'boolean', 'Inline project expressions into values'], ['min_hash_partition_count', '4', '4', 'integer', 'Minimum number of partitions for distributed joins and aggregations'], ['min_hash_partition_count_for_write', '50', '50', 'integer', 'Minimum number of partitions for distributed joins and aggregations in write queries'], ['min_input_rows_per_task', '10000000', '10000000', 'bigint', 'Minimum input rows required per task. This will help optimizer determine hash partition count for joins and aggregations'], ['min_input_size_per_task', '5GB', '5GB', 'varchar', 'Minimum input data size required per task. This will help optimizer determine hash partition count for joins and aggregations'], ['non_estimatable_predicate_approximation_enabled', 'true', 'true', 'boolean', 'Approximate the cost of filters which cannot be accurately estimated even with complete statistics'], ['omit_datetime_type_precision', 'false', 'false', 'boolean', 'Omit precision when rendering datetime type names with default precision'], ['optimize_duplicate_insensitive_joins', 'true', 'true', 'boolean', 'Optimize duplicate insensitive joins'], ['optimize_hash_generation', 'true', 'true', 'boolean', 'Compute hash codes for distribution, joins, and aggregations early in query plan'], ['optimize_metadata_queries', 'false', 'false', 'boolean', 'Enable optimization for metadata queries'], ['optimize_mixed_distinct_aggregations', 'false', 'false', 'boolean', 'Optimize mixed non-distinct and distinct aggregations'], ['optimize_top_n_ranking', 'true', 'true', 'boolean', 'Use top N ranking optimization'], ['parse_decimal_literals_as_double', 'false', 'false', 'boolean', 'Parse decimal literals as DOUBLE instead of DECIMAL'], ['pre_aggregate_case_aggregations_enabled', 'true', 'true', 'boolean', 'Pre-aggregate rows before GROUP BY with multiple CASE aggregations on same column'], ['predicate_pushdown_use_table_properties', 'true', 'true', 'boolean', 'Use table properties in predicate pushdown'], ['prefer_partial_aggregation', 'true', 'true', 'boolean', 'Prefer splitting aggregations into partial and final stages'], ['prefer_streaming_operators', 'false', 'false', 'boolean', 'Prefer source table layouts that produce streaming operators'], ['push_aggregation_through_outer_join', 'true', 'true', 'boolean', 'Allow pushing aggregations below joins'], ['push_partial_aggregation_through_join', 'false', 'false', 'boolean', 'Push partial aggregations below joins'], ['push_table_write_through_union', 'true', 'true', 'boolean', 'Parallelize writes when using UNION ALL in queries that write data'], ['query_max_cpu_time', '1000000000.00d', '1000000000.00d', 'varchar', 'Maximum CPU time of a query'], ['query_max_execution_time', '100.00d', '100.00d', 'varchar', 'Maximum execution time of a query'], ['query_max_planning_time', '10.00m', '10.00m', 'varchar', 'Maximum planning time of a query'], ['query_max_run_time', '100.00d', '100.00d', 'varchar', 'Maximum run time of a query (includes the queueing time)'], ['query_max_scan_physical_bytes', '', '', 'varchar', 'Maximum scan physical bytes of a query'], ['query_priority', '1', '1', 'integer', 'The priority of queries. Larger numbers are higher priority'], ['query_retry_attempts', '4', '4', 'integer', 'Maximum number of query retry attempts'], ['redistribute_writes', 'true', 'true', 'boolean', 'Force parallel distributed writes'], ['remote_task_adaptive_update_request_size_enabled', 'true', 'true', 'boolean', 'Experimental: Enable adaptive adjustment for size of remote task update request'], ['remote_task_guaranteed_splits_per_request', '3', '3', 'integer', 'Guaranteed splits per remote task request'], ['remote_task_max_request_size', '8MB', '8MB', 'varchar', 'Experimental: Max size of remote task update request'], ['remote_task_request_size_headroom', '2MB', '2MB', 'varchar', 'Experimental: Headroom for size of remote task update request'], ['required_workers_count', '1', '1', 'integer', 'Minimum number of active workers that must be available before the query will start'], ['required_workers_max_wait_time', '5.00m', '5.00m', 'varchar', 'Maximum time to wait for minimum number of workers before the query is failed'], ['resource_overcommit', 'false', 'false', 'boolean', 'Use resources which are not guaranteed to be available to the query'], ['retry_delay_scale_factor', '2.0', '2.0', 'double', \"Maximum delay before initiating a retry attempt. Delay increases exponentially for each subsequent attempt starting from 'retry_initial_delay'\"], ['retry_initial_delay', '10.00s', '10.00s', 'varchar', \"Initial delay before initiating a retry attempt. Delay increases exponentially for each subsequent attempt up to 'retry_max_delay'\"], ['retry_max_delay', '1.00m', '1.00m', 'varchar', \"Maximum delay before initiating a retry attempt. Delay increases exponentially for each subsequent attempt starting from 'retry_initial_delay'\"], ['rewrite_filtering_semi_join_to_inner_join', 'true', 'true', 'boolean', 'Rewrite semi join in filtering context to inner join'], ['scale_writers', 'true', 'true', 'boolean', 'Scale out writers based on throughput (use minimum necessary)'], ['skip_redundant_sort', 'true', 'true', 'boolean', 'Skip redundant sort operations'], ['spatial_join', 'true', 'true', 'boolean', 'Use spatial index for spatial join when possible'], ['spatial_partitioning_table_name', '', '', 'varchar', 'Name of the table containing spatial partitioning scheme'], ['spill_enabled', 'false', 'false', 'boolean', 'Enable spilling'], ['split_concurrency_adjustment_interval', '100.00ms', '100.00ms', 'varchar', 'Experimental: Interval between changes to the number of concurrent splits per node'], ['statistics_cpu_timer_enabled', 'true', 'true', 'boolean', 'Experimental: Enable cpu time tracking for automatic column statistics collection on write'], ['statistics_precalculation_for_pushdown_enabled', 'true', 'true', 'boolean', 'Enable statistics precalculation for pushdown'], ['table_scan_node_partitioning_min_bucket_to_task_ratio', '0.5', '0.5', 'double', 'Min table scan bucket to task ratio for which plan will be adopted to node pre-partitioned tables'], ['task_concurrency', '4', '32', 'integer', 'Default number of local parallel jobs per worker'], ['task_partitioned_writer_count', '32', '32', 'integer', 'Number of local parallel table writers per task when prefer partitioning is used'], ['task_retry_attempts_per_task', '4', '4', 'integer', 'Maximum number of task retry attempts per single task'], ['task_scale_writers_enabled', 'true', 'true', 'boolean', 'Scale the number of concurrent table writers per task based on throughput'], ['task_share_index_loading', 'false', 'false', 'boolean', 'Share index join lookups and caching within a task'], ['task_writer_count', '1', '1', 'integer', 'Number of local parallel table writers per task when prefer partitioning and task writer scaling are not used'], ['use_cost_based_partitioning', 'true', 'true', 'boolean', 'When enabled the cost based optimizer is used to determine if repartitioning the output of an already partitioned stage is necessary'], ['use_exact_partitioning', 'false', 'false', 'boolean', 'When enabled this forces data repartitioning unless the partitioning of upstream stage matches exactly what downstream stage expects'], ['use_legacy_window_filter_pushdown', 'false', 'false', 'boolean', 'Use legacy window filter pushdown optimizer'], ['use_mark_distinct', '', '', 'boolean', 'Implement DISTINCT aggregations using MarkDistinct'], ['use_preferred_write_partitioning', 'true', 'true', 'boolean', 'Use preferred write partitioning'], ['use_table_scan_node_partitioning', 'true', 'true', 'boolean', 'Adapt plan to node pre-partitioned tables'], ['writer_min_size', '32MB', '32MB', 'varchar', 'Target minimum size of writer output when scaling writers'], ['hive.bucket_execution_enabled', 'true', 'true', 'boolean', 'Enable bucket-aware execution: only use a single worker per bucket'], ['hive.collect_column_statistics_on_write', 'true', 'true', 'boolean', 'Enables automatic column level statistics collection on write'], ['hive.compression_codec', 'GZIP', 'GZIP', 'varchar', 'Compression codec to use when writing files. Possible values: [NONE, SNAPPY, LZ4, ZSTD, GZIP]'], ['hive.create_empty_bucket_files', 'true', 'true', 'boolean', 'Create empty files for buckets that have no data'], ['hive.csv_native_reader_enabled', 'true', 'true', 'boolean', 'Use native CSV reader'], ['hive.csv_native_writer_enabled', 'true', 'true', 'boolean', 'Use native CSV writer'], ['hive.dynamic_filtering_wait_timeout', '0.00m', '0.00m', 'varchar', 'Duration to wait for completion of dynamic filters during split generation'], ['hive.force_local_scheduling', 'false', 'false', 'boolean', 'Only schedule splits on workers colocated with data node'], ['hive.hive_storage_format', 'ORC', 'ORC', 'varchar', 'Default storage format for new tables or partitions. Possible values: [ORC, PARQUET, AVRO, RCBINARY, RCTEXT, SEQUENCEFILE, JSON, OPENX_JSON, TEXTFILE, CSV, REGEX]'], ['hive.hive_views_legacy_translation', 'false', 'false', 'boolean', 'Use legacy Hive view translation mechanism'], ['hive.ignore_absent_partitions', 'false', 'false', 'boolean', 'Ignore partitions when the file system location does not exist rather than failing the query.'], ['hive.ignore_corrupted_statistics', 'false', 'false', 'boolean', 'Experimental: Ignore corrupted statistics rather than failing'], ['hive.insert_existing_partitions_behavior', 'APPEND', 'APPEND', 'varchar', \"Behavior on insert existing partitions; this session property doesn't control behavior on insert existing unpartitioned table\"], ['hive.json_native_reader_enabled', 'true', 'true', 'boolean', 'Use native JSON reader'], ['hive.json_native_writer_enabled', 'true', 'true', 'boolean', 'Use native JSON writer'], ['hive.minimum_assigned_split_weight', '0.05', '0.05', 'double', 'Minimum assigned split weight when size based split weighting is enabled'], ['hive.non_transactional_optimize_enabled', 'false', 'false', 'boolean', 'Enable OPTIMIZE table procedure'], ['hive.openx_json_native_reader_enabled', 'true', 'true', 'boolean', 'Use native OpenX JSON reader'], ['hive.openx_json_native_writer_enabled', 'true', 'true', 'boolean', 'Use native OpenX JSON writer'], ['hive.optimize_mismatched_bucket_count', 'false', 'false', 'boolean', 'Experimental: Enable optimization to avoid shuffle when bucket count is compatible but not the same'], ['hive.optimize_symlink_listing', 'true', 'true', 'boolean', 'Optimize listing for SymlinkTextFormat tables with files in a single directory'], ['hive.orc_bloom_filters_enabled', 'false', 'false', 'boolean', 'ORC: Enable bloom filters for predicate pushdown'], ['hive.orc_lazy_read_small_ranges', 'true', 'true', 'boolean', 'Experimental: ORC: Read small file segments lazily'], ['hive.orc_max_buffer_size', '8MB', '8MB', 'varchar', 'ORC: Maximum size of a single read'], ['hive.orc_max_merge_distance', '1MB', '1MB', 'varchar', 'ORC: Maximum size of gap between two reads to merge into a single read'], ['hive.orc_max_read_block_size', '16MB', '16MB', 'varchar', 'ORC: Soft max size of Trino blocks produced by ORC reader'], ['hive.orc_nested_lazy_enabled', 'true', 'true', 'boolean', 'Experimental: ORC: Lazily read nested data'], ['hive.orc_optimized_writer_max_dictionary_memory', '16MB', '16MB', 'varchar', 'ORC: Max dictionary memory'], ['hive.orc_optimized_writer_max_stripe_rows', '10000000', '10000000', 'integer', 'ORC: Max stripe row count'], ['hive.orc_optimized_writer_max_stripe_size', '64MB', '64MB', 'varchar', 'ORC: Max stripe size'], ['hive.orc_optimized_writer_min_stripe_size', '32MB', '32MB', 'varchar', 'ORC: Min stripe size'], ['hive.orc_optimized_writer_validate', 'false', 'false', 'boolean', 'ORC: Force all validation for files'], ['hive.orc_optimized_writer_validate_mode', 'BOTH', 'BOTH', 'varchar', 'ORC: Level of detail in ORC validation. Possible values: [HASHED, DETAILED, BOTH]'], ['hive.orc_optimized_writer_validate_percentage', '0.0', '0.0', 'double', 'ORC: sample percentage for validation for files'], ['hive.orc_stream_buffer_size', '8MB', '8MB', 'varchar', 'ORC: Size of buffer for streaming reads'], ['hive.orc_string_statistics_limit', '64B', '64B', 'varchar', 'ORC: Maximum size of string statistics; drop if exceeding'], ['hive.orc_tiny_stripe_threshold', '8MB', '8MB', 'varchar', 'ORC: Threshold below which an ORC stripe or file will read in its entirety'], ['hive.orc_use_column_names', 'false', 'false', 'boolean', 'ORC: Access ORC columns using names from the file'], ['hive.parallel_partitioned_bucketed_writes', 'true', 'true', 'boolean', 'Improve parallelism of partitioned and bucketed table writes'], ['hive.parquet_ignore_statistics', 'false', 'false', 'boolean', 'Ignore statistics from Parquet to allow querying files with corrupted or incorrect statistics'], ['hive.parquet_max_read_block_row_count', '8192', '8192', 'integer', 'Parquet: Maximum number of rows read in a batch'], ['hive.parquet_max_read_block_size', '16MB', '16MB', 'varchar', 'Parquet: Maximum size of a block to read'], ['hive.parquet_optimized_nested_reader_enabled', 'true', 'true', 'boolean', 'Use optimized Parquet reader for nested columns'], ['hive.parquet_optimized_reader_enabled', 'true', 'true', 'boolean', 'Use optimized Parquet reader'], ['hive.parquet_optimized_writer_enabled', 'false', 'false', 'boolean', 'Enable optimized writer'], ['hive.parquet_optimized_writer_validation_percentage', '5.0', '5.0', 'double', 'Parquet: sample percentage for validation of written files'], ['hive.parquet_use_bloom_filter', 'true', 'true', 'boolean', 'Use Parquet bloomfilter'], ['hive.parquet_use_column_index', 'true', 'true', 'boolean', 'Use Parquet column index'], ['hive.parquet_use_column_names', 'true', 'true', 'boolean', 'Parquet: Access Parquet columns using names from the file'], ['hive.parquet_writer_batch_size', '10000', '10000', 'integer', 'Parquet: Maximum number of rows passed to the writer in each batch'], ['hive.parquet_writer_block_size', '134217728B', '134217728B', 'varchar', 'Parquet: Writer block size'], ['hive.parquet_writer_page_size', '1048576B', '1048576B', 'varchar', 'Parquet: Writer page size'], ['hive.partition_statistics_sample_size', '100', '100', 'integer', 'Maximum sample size of the partitions column statistics'], ['hive.projection_pushdown_enabled', 'true', 'true', 'boolean', 'Projection push down enabled for hive'], ['hive.propagate_table_scan_sorting_properties', 'false', 'false', 'boolean', 'Use sorted table layout to generate more efficient execution plans. May lead to incorrect results if files are not sorted as per table definition.'], ['hive.query_partition_filter_required', 'false', 'false', 'boolean', 'Require filter on partition column'], ['hive.query_partition_filter_required_schemas', '[]', '[]', 'array(varchar)', 'List of schemas for which filter on partition column is enforced.'], ['hive.rcfile_optimized_writer_validate', 'false', 'false', 'boolean', 'RCFile: Validate writer files'], ['hive.regex_native_reader_enabled', 'true', 'true', 'boolean', 'Use native REGEX reader'], ['hive.respect_table_format', 'true', 'true', 'boolean', 'Write new partitions using table format rather than default storage format'], ['hive.s3_select_pushdown_enabled', 'true', 'true', 'boolean', 'S3 Select pushdown enabled'], ['hive.sequence_file_native_reader_enabled', 'true', 'true', 'boolean', 'Use native sequence file reader'], ['hive.sequence_file_native_writer_enabled', 'true', 'true', 'boolean', 'Use native sequence file writer'], ['hive.size_based_split_weights_enabled', 'true', 'true', 'boolean', 'Enable estimating split weights based on size in bytes'], ['hive.sorted_writing_enabled', 'true', 'true', 'boolean', 'Enable writing to bucketed sorted tables'], ['hive.statistics_enabled', 'true', 'true', 'boolean', 'Expose table statistics'], ['hive.target_max_file_size', '1GB', '1GB', 'varchar', 'Target maximum size of written files; the actual size may be larger'], ['hive.temporary_staging_directory_enabled', 'true', 'true', 'boolean', 'Should use temporary staging directory for write operations'], ['hive.temporary_staging_directory_path', '/tmp/presto-${USER}', '/tmp/presto-${USER}', 'varchar', 'Temporary staging directory location'], ['hive.text_file_native_reader_enabled', 'true', 'true', 'boolean', 'Use native text file reader'], ['hive.text_file_native_writer_enabled', 'true', 'true', 'boolean', 'Use native text file writer'], ['hive.timestamp_precision', 'MILLISECONDS', 'MILLISECONDS', 'varchar', 'Precision for timestamp columns in Hive tables. Possible values: [MILLISECONDS, MICROSECONDS, NANOSECONDS]'], ['hive.validate_bucketing', 'true', 'true', 'boolean', 'Verify that data is bucketed correctly when reading'], ['tpcds.split_count', '', '', 'integer', \"Number of split to be created. If not specified the number of splits is computed as 'splits_per_node * <number of active nodes>'\"], ['tpcds.splits_per_node', '4', '4', 'integer', 'Number of splits created for each worker node'], ['tpcds.with_no_sexism', 'false', 'false', 'boolean', 'With no sexism']]\n",
      "Error executing Trino query: TrinoQueryError(type=INSUFFICIENT_RESOURCES, name=EXCEEDED_LOCAL_MEMORY_LIMIT, message=\"Query exceeded per-node memory limit of 22GB [Allocated: 22.00GB, Delta: 559.30kB, Top Consumers: {HashBuilderOperator=21.85GB, PagePartitioner=62.81MB, ExchangeOperator=33.74MB}]\", query_id=20230509_084639_00033_3uc2t)\n"
     ]
    }
   ],
   "source": [
    " for key in s3_result['Contents']:\n",
    "        key_path = key['Key']\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=key_path)\n",
    "        file_content = response['Body'].read().decode('utf-8')\n",
    "        print(key_path)\n",
    "        file_content = file_content.replace(\"${baseLocation}\", \"s3a://\" + bucket_name)\n",
    "        for query in file_content.split(\";\"):\n",
    "                if(len(query) > 5):\n",
    "                    query = query.replace(\"EXTERNAL\", \"\")\n",
    "                    #print(\"------query\", query)\n",
    "                    \n",
    "                    try:\n",
    "                        cur.execute(query)\n",
    "                        rows = cur.fetchall()\n",
    "                        print(rows)\n",
    "                    except trino.dbapi.Error as e:\n",
    "                        print(f'Error executing Trino query: {e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
